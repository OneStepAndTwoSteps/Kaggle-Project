{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <h1><center> Predictive Analysis - Porto Seguro’s Safe Driver Prediction | Kaggle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <h1><center> I. Importation & Missing Value Check","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py     # 绘图的函数\n\n\nimport plotly.graph_objs as go              # 可用于绘制不同图型，如 go.bar()\nimport plotly.express as px                 # 可用于绘制不同图型，如 px.bar()\nfrom plotly.subplots import make_subplots   # 创建子图\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)    #THIS LINE IS MOST IMPORTANT AS THIS WILL DISPLAY PLOT ON \n#NOTEBOOK WHILE KERNEL IS RUNNING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n\n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n\n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n\n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n\n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n            \" columns that have missing values.\")\n\n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(train_copy)  # train_copy 是一个 dataframe\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出 `ps_car_03_cat` 和 `ps_car_05_cat` 所占缺失值比例很高","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <center>II. Data Cleaning & Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_counts = train.target.value_counts()\n# train_counts = pd.DataFrame(train_counts)\n\n# fig = px.bar(train_counts,x=train_counts.index,y='target',barmode='group',color='target')\n# fig.update_traces(textposition='outside')\n# fig.update_layout(template='seaborn',title='target (counts)')\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 目标变量检测\n\n对于分类变量我们需要进行目标变量检测，如果数据存在严重的不平衡，预测得出的结论往往也是有偏的，即分类结果会偏向于较多观测的类。\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_counts = train.target.value_counts()\ntrain_counts = pd.DataFrame(train_counts)\n\nfig = px.bar(train_counts,x=train_counts.index,y='target',barmode='group',color='target',text='target') # text 可以标上数值\nfig.update_traces(textposition='outside')\nfig.update_layout(yaxis_title='counts',xaxis_title='target',template='seaborn',title='target (counts)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 二进制数据检测 `bin`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们能看到，对于`10_bin` `11_bin` `12_bin` `13_bin` 基本上全是目标值都为 0，那么我们可以初步判断这几个特征可能对我们的目标值预测起不了作用，等一下我们也可以再进一步的去验证。","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 数据不平衡\n\n可以发现标签之前存在不平衡的状态，如果数据存在严重的不平衡，预测得出的结论往往也是有偏的，即分类结果会偏向于较多观测的类。\n\n比如我们使用准确率来进行模型的评估，即使我们全部预测成 `target == 0`，那么也有很高的准确率: `573518/(573518+21694)=0.96`。\n\n所以对于分类不平衡的数据，我们可以进行如下操作：\n\n### 欠采样：\n\n随机欠采样（下采样）的目标是通过随机地消除占多数的类的样本来平衡类分布；直到多数类和少数类的实例实现平衡，目标才算达成。\n\n* `随机欠采样（下采样）`的目标是通过随机地消除占多数的类的样本来平衡类分布；直到多数类和少数类的实例实现平衡，目标才算达成。\n\n* `随机下采样的优点：`\n    \n    它可以提升运行时间；并且当训练数据集很大时，可以通过减少样本数量来解决存储问题。\n\n* `随机下采样的缺点：`\n    \n    它会丢弃对构建规则分类器很重要的有价值的潜在信息。\n\n    被随机欠采样选取的样本可能具有偏差。它不能准确代表大多数。从而在实际的测试数据集上得到不精确的结果。果。\n\n\n### 过采样\n\n\n* `随机过采样` 通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。\n\n* `随机过采样的优点：`\n\n    与欠采样不同，这种方法不会带来信息损失。\n\n    表现优于欠采样。\n\n* `随机过采样的缺点：`\n    \n    由于复制少数类事件，它加大了过拟合的可能性。\n    \n\n本 notebook 将采用 SMOTE 来进行过采样","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 不进行采样，直接进行模型训练和预测查看模型质量\n\n* 通过对比准确度和召回率\n    \n* 通过绘制混淆矩阵","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1、对比准确度和召回率","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = train['target']\n# train_feature = train.drop(columns='target')\ntrain_feature = train.drop(columns = ['target','id'])\n\n\nx_train,x_test,y_train,y_test = train_test_split(train_feature,train_target,test_size= 0.2,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = XGBClassifier(n_estimators=1000)\nmodel = XGBClassifier()\n\n\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\n\nprint('Accuracy: {:.3f}'.format(acc* 100.0))\nprint('recall: {:.3f}'.format(recall* 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到，即使你的准确率非常高，但是召回率却非常低    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2、绘制混淆矩阵\n\n混淆矩阵用法示例，用于评估数据集上分类器输出的质量。 对角线元素表示预测标签等于真实标签的点数，而非对角线元素则是分类器未正确标记的元素。 混淆矩阵的对角线值越高，表示对数越多越好。混淆矩阵用法示例，用于评估数据集上分类器输出的质量。 对角线元素表示预测标签等于真实标签的点数，而非对角线元素则是分类器未正确标记的元素。 混淆矩阵的对角线值越高，表示对数越多越好。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                                normalize=False,\n                                title='Confusion matrix',\n                                cmap=plt.cm.Blues):\n        \"\"\"\n            此函数打印并绘制混淆矩阵。\n            可以通过设置“ normalize = True”来应用归一化。\n        \"\"\"\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n\n        print(cm)\n\n        thresh = cm.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n                        horizontalalignment=\"center\",\n                        color=\"white\" if cm[i, j] > thresh else \"black\")\n\n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['target_0','target_1'] # 顺序别搞错\nnp.set_printoptions(precision=2)\n\ncm = confusion_matrix(y_test,y_pred)\nplt.figure()\nplot_confusion_matrix(cm,classes)\nplt.show()\n\n# plt.figure()\n# plot_confusion_matrix(cm,classes,normalize=True)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 使用 SMOTE 进行过采样","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 查看2D数据的分布","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 定义绘图函数\ndef plot_2d_space(X, y, label='Classes'):   \n            colors = ['#1F77B4', '#FF7F0E']\n            markers = ['o', 's']\n            for l, c, m in zip(np.unique(y), colors, markers):\n                plt.scatter(\n                    X[y==l, 0],\n                    X[y==l, 1],\n                    c=c, label=l, marker=m\n                )\n            plt.title(label)\n            plt.legend(loc='upper right')\n            plt.show()\n            \nprint(\"label0: \",len(x_train[y_train==0]))\nprint(\"label1: \",len(x_train[y_train==1]))\n\nss = StandardScaler()\nX = ss.fit_transform(x_train)\n\n# `2、`如果数据存在多维特征可使用PCA来降维，使其能在2D图中展示\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y_train, 'Imbalanced dataset (2 PCA components)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampler=SMOTE(random_state=0)\n# 开始人工合成数据\nos_features,os_labels=oversampler.fit_sample(x_train,y_train)\n\n# 查看生成结果\nprint(\"label1: \",len(os_labels[os_labels==1]))\nprint(\"label0: \",len(os_labels[os_labels==0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampler=SMOTE(random_state=0)\n# 开始人工合成数据\nos_features_test,os_labels_test=oversampler.fit_sample(x_test,y_test)\n\n# 查看生成结果\nprint(\"label1: \",len(os_labels_test[os_labels_test==1]))\nprint(\"label0: \",len(os_labels_test[os_labels_test==0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = StandardScaler()\nX = ss.fit_transform(os_features)\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, os_labels, 'Imbalanced dataset (2 PCA components)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 特征相关性检测","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1、特征和目标特征之间的相关性检测","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_os_features = os_features.copy()\nnew_os_features['target'] = os_labels\n\n# Find correlations with the target and sort\ncorrs = new_os_features.corr()['target'].sort_values(ascending=False)\ncorrelations = pd.DataFrame(corrs)\n\n# Display correlations\nprint('Most Positive Correlations:\\n')\ncorrelations.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Most Negative Correlations:\\n')\ncorrelations.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations.loc[correlations.index.isin(['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin','ps_ind_13_bin'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(corrs).sort_values(ascending=False).tail(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们可以看出 在 `bin` 类型的特征中 `ps_ind_10_bin`,`ps_ind_11_bin`,`ps_ind_12_bin`,`ps_ind_13_bin` 确实是影响力最低的特征","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"2、特征与特征之间进行相关性检测","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = os_features.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3、删除共线特征","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 设置阈值\nthreshold = 0.8\n\n# 创建一个空字典以容纳相关变量\nabove_threshold_vars = {}\n\n# 对于每一列，记录index行中的那个值高于阈值的变量\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above_threshold_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 跟踪要删除的列和已检查的列\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# 遍历列和相关列\nfor key, value in above_threshold_vars.items():\n    # 跟踪已检查的列\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # 如果存在高相关的特征，只保留一个\n            if x not in cols_seen:                  # 如果该特征在之前的 key 数据中没有出现过。\n                cols_to_remove.append(x)            # 存在高度相关，将高度相关的特征放入 cols_to_remove 中。\n                cols_to_remove_pair.append(key)     # cols_to_remove 和 cols_to_remove_pair 得到的结果一致。\n\ncols_to_remove = list(set(cols_to_remove))\nprint('Name of columns to remove: ', cols_to_remove)\nprint('Number of columns to remove: ', len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed = os_features.drop(columns = cols_to_remove)\ntest_corrs_removed = os_features_test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed.to_csv('train_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_corrs_removed.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center>III. ML Approach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### XGBoost训练模型\n\n使用 gini 系数评估 + 交叉验证","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `1、定义基尼系数：`\n\ndef gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n    gs -= (len(y) + 1) / 2.\n    return gs / len(y)\n\n# `2、定义 xgb gini 系数：`\n\n# 返回一个 normalized 后的 gini 分数\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) / gini(y, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 过采样前数据进行训练","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\nnrounds=200  \nkfold = 2\nskf = StratifiedKFold(n_splits=kfold, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(skf.split(train_feature, train_target)):\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = train_feature.loc[train_index], train_feature.loc[test_index]\n    y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n    d_train = xgb.DMatrix(X_train, y_train) \n    d_valid = xgb.DMatrix(X_valid, y_valid) \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n                          feval=gini_xgb, maximize=True, verbose_eval=100)\n    \n# xgb_y_pred = xgb_model.predict(xgb.DMatrix(test[features].values), \n#                         ntree_limit=xgb_model.best_ntree_limit+50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train_feature.columns\ntest_ids = test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_test_predictions1 = xgb_model.predict(xgb.DMatrix(test[features]),ntree_limit=xgb_model.best_ntree_limit+50)\n\nsubmission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions1})\nsubmission.to_csv('before_oversampling_xgb.csv', index = False, float_format='%.5f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Kfold = 2` and `nrounds = 200` 提交后：`score=0.254` \n\n`Kfold = 5` and `nrounds = 2000` 提交后：`score=0.2767` ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 特征选择\n\n绘制 xgb 模型特征的重要性图","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_feature_importances(model):\n    trace = go.Scatter(\n        y = np.array(list(model.get_fscore().values())),\n        x = np.array(list(model.get_fscore().keys())),\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 13,\n            #size= model.feature_importances_,\n            #color = np.random.randn(500), #set color equal to a variable\n            color =  np.array(list(model.get_fscore().values())),\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = np.array(list(model.get_fscore().keys()))\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= 'xgb Feature Importance',\n        hovermode= 'closest',\n         xaxis= dict(\n             ticklen= 5,\n             showgrid=False,\n            zeroline=False,\n            showline=False\n         ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            showgrid=False,\n            zeroline=False,\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_feature_importances(xgb_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"更近一步，我们还能通过绘制柱状图(横)来对特征重要性进行效果展示","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_importance = np.array(list(xgb_model.get_fscore().values()))\nxgb_features = np.array(list(xgb_model.get_fscore().keys()))\n\nx, y = (list(x) for x in zip(*sorted(zip(xgb_importance,xgb_features), reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n    width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"从图中我们能发现个别特征重要性非常低，我们可以进行剔除","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = pd.DataFrame(xgb_model.get_fscore(),index=['features_importance']).T\\\n                        .sort_values(by='features_importance',ascending=False)\\\n                        .iloc[0:38]\nnew_features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_feature = train_feature[new_features.index]\nnew_train_feature.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\nnrounds=200  \nkfold = 2\nskf = StratifiedKFold(n_splits=kfold, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(skf.split(new_train_feature, train_target)):\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n    y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n    d_train = xgb.DMatrix(X_train, y_train) \n    d_valid = xgb.DMatrix(X_valid, y_valid) \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    xgb_model2 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n                          feval=gini_xgb, maximize=True, verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_test_predictions2 = xgb_model2.predict(xgb.DMatrix(test[new_features.index]),ntree_limit=xgb_model2.best_ntree_limit+50)\n\nsubmission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions2})\nsubmission.to_csv('before_oversampling_after_feature_choose_xgb.csv', index = False, float_format='%.5f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### xgb + lgb 堆叠训练预测\n\n对选出来的特征进行训练","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) / gini(y, y)\n    return 'gini', score, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282\n\n# xgb\n# params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n#         'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\n# submission=test['id'].to_frame()\n# submission['target']=0\n\n# nrounds=200  # need to change to 2000\n# kfold = 2  # need to change to 5\n# skf = StratifiedKFold(n_splits=kfold, random_state=0)\n# for i, (train_index, test_index) in enumerate(skf.split(new_train_feature, train_target)):\n#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_valid = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n#     y_train, y_valid = train_target.loc[train_index], train_target.loc[test_index]\n#     d_train = xgb.DMatrix(X_train, y_train) \n#     d_valid = xgb.DMatrix(X_valid, y_valid) \n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#     xgb_model3 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n#                         feval=gini_xgb, maximize=True, verbose_eval=100)\n\n#     # 结尾 除以 (2*kfold)，是因为要将 xgb 和 lgb 去平均然后将结果相加合并\n#     submission['target'] += xgb_model3.predict(xgb.DMatrix(test[new_features.index]), \n#                         ntree_limit=xgb_model3.best_ntree_limit+50) / (2*kfold)\n    \n# submission.head(2)\n\n# # lgb\n# params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':10, 'max_bin':10,  'objective': 'binary', \n#         'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\n\n# skf = StratifiedKFold(n_splits=kfold, random_state=1)\n# for i, (train_index, test_index) in enumerate(skf.split(new_train_feature, os_labels)):\n#     print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_eval = new_train_feature.loc[train_index], new_train_feature.loc[test_index]\n#     y_train, y_eval = train_target.loc[train_index], train_target.loc[test_index]\n#     lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds, \n#                 lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, \n#                 feval=gini_lgb, early_stopping_rounds=100)\n\n#     # 结尾 除以 (2*kfold)，是因为要将 xgb 和 lgb 去平均然后将结果相加合并\n#     submission['target'] += lgb_model.predict(test[new_features.index], \n#                         num_iteration=lgb_model.best_iteration) / (2*kfold)\n\n# submission.to_csv('before_oversampling_lgb+xgb.csv', index=False, float_format='%.5f') \n\n# submission.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission.to_csv('before_oversampling_after_feature_choose_xgb.csv', index = False, float_format='%.5f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 过采样后数据进行训练\n\noversampling good or bad?\n\n对采样后的数据进行训练，发现预测出来的结果分数很低","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n#           'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n\n# nrounds=200  \n# kfold = 2  \n# skf = StratifiedKFold(n_splits=kfold, random_state=0)\n\n# for i, (train_index, test_index) in enumerate(skf.split(os_features, os_labels)):\n#     print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n#     X_train, X_valid = os_features.loc[train_index], os_features.loc[test_index]\n#     y_train, y_valid = os_labels.loc[train_index], os_labels.loc[test_index]\n#     d_train = xgb.DMatrix(X_train, y_train) \n#     d_valid = xgb.DMatrix(X_valid, y_valid) \n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#     xgb_model2 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n#                           feval=gini_xgb, maximize=True, verbose_eval=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_test_predictions = xgb_model2.predict(xgb.DMatrix(test[features]), \n#                         ntree_limit=xgb_model2.best_ntree_limit+50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.DataFrame({'id': test_ids, 'target': xgb_test_predictions})\n# submission.to_csv('after_oversampling_xgb.csv', index = False, float_format='%.5f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> IV. Extend...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier 训练数据\n\n接下来我们将：\n\n* 1、使用 RandomForestClassifier 来进行训练数据\n\n* 2、绘制特征的重要性图\n\n* 3、并且进一步进行特征选择","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os_features.drop(['id'],axis=1,inplace=True)\n# os_features_test.drop(['id'],axis=1,inplace=True)\n\n\n# rdf_clf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n# rdf_clf.fit(os_features, os_labels)\n# features = os_features.columns.values\n# print(\"----- Training Done -----\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestClassifier feature importances Scatter plot \n\n# def rdf_feature_importances(model):\n#     trace = go.Scatter(\n#         y = model.feature_importances_,\n#         x = features,\n#         mode='markers',\n#         marker=dict(\n#             sizemode = 'diameter',\n#             sizeref = 1,\n#             size = 13,\n#             #size= model.feature_importances_,\n#             #color = np.random.randn(500), #set color equal to a variable\n#             color = model.feature_importances_,\n#             colorscale='Portland',\n#             showscale=True\n#         ),\n#         text = features\n#     )\n#     data = [trace]\n\n#     layout= go.Layout(\n#         autosize= True,\n#         title= 'Random Forest Feature Importance',\n#         hovermode= 'closest',\n#          xaxis= dict(\n#              ticklen= 5,\n#              showgrid=False,\n#             zeroline=False,\n#             showline=False\n#          ),\n#         yaxis=dict(\n#             title= 'Feature Importance',\n#             showgrid=False,\n#             zeroline=False,\n#             ticklen= 5,\n#             gridwidth= 2\n#         ),\n#         showlegend= False\n#     )\n#     fig = go.Figure(data=data, layout=layout)\n#     fig.show()\n\n# rdf_feature_importances(model=rdf_clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"更近一步，我们还能通过绘制柱状图(横)来对特征重要性进行效果展示","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# x, y = (list(x) for x in zip(*sorted(zip(rdf_clf.feature_importances_, features), \n#                                                             reverse = False)))\n# trace2 = go.Bar(\n#     x=x ,\n#     y=y,\n#     marker=dict(\n#         color=x,\n#         colorscale = 'Viridis',\n#         reversescale = True\n#     ),\n#     name='Random Forest Feature importance',\n#     orientation='h',\n# )\n\n# layout = dict(\n#     title='Barplot of Feature importances',\n#      width = 900, height = 2000,\n#     yaxis=dict(\n#         showgrid=False,\n#         showline=False,\n#         showticklabels=True,\n# #         domain=[0, 0.85],\n#     ))\n\n# fig1 = go.Figure(data=[trace2])\n# fig1['layout'].update(layout)\n# py.iplot(fig1, filename='plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = rdf_clf.predict(os_features_test)\n\n# acc = accuracy_score(os_labels_test,y_pred)\n# recall = recall_score(os_labels_test,y_pred)\n\n# print('Accuracy: {:.3f}'.format(acc* 100.0))\n# print('recall: {:.3f}'.format(recall* 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = XGBClassifier()\n\n\n# model.fit(os_features,os_labels)\n# y_pred = model.predict(os_features_test)\n\n# acc = accuracy_score(os_labels_test,y_pred)\n# recall = recall_score(os_labels_test,y_pred)\n\n# print('Accuracy: {:.3f}'.format(acc* 100.0))\n# print('recall: {:.3f}'.format(recall* 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们可以看出相比于之前的不平衡数据，过采样后的数据，召回率得到了明显的上升","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 对数据进行预测\n\n使用 交叉验证 放法进行模型训练","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"随机森林预测结果：","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_predictions = rdf_clf.predict(new_test)\n\n# submission = pd.DataFrame({'id': test_ids, 'target': test_predictions})\n# submission.to_csv('RandomForestClassifier_predict_1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 特征选择","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}