{"cells":[{"metadata":{"_uuid":"9d7057b0f4ce0d2bb50bfea37c29ce0927cdf53f"},"cell_type":"markdown","source":"# Introduction: Manual Feature Engineering\n\nIf you are new to this competition, I highly suggest checking out [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) to get started.\n\nIn this notebook, we will explore making features by hand for the Home Credit Default Risk competition. In an earlier notebook, we used only the `application` data in order to build a model. The best model we made from this data achieved a score on the leaderboard around 0.74. In order to better this score, we will have to include more information from the other dataframes. Here, we will look at using information from the `bureau` and `bureau_balance` data. The definitions of these data files are:\n\n* bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.\n* bureau_balance: monthly information about the previous loans. Each month has its own row.\n\nManual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. Since I have limited domain knowledge of loans and what makes a person likely to default, I will instead concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA. \n\nThe process of manual feature engineering will involve plenty of Pandas code, a little patience, and a lot of great practice manipulation data. Even though automated feature engineering tools are starting to be made available, feature engineering will still have to be done using plenty of data wrangling for a little while longer. ","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4e0980ff7f4d8d9f0661b5d5ebf07e66d304222"},"cell_type":"markdown","source":"## Example: Counts of a client's previous loans\n\nTo illustrate the general process of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions. This requires a number of Pandas operations we will make heavy use of throughout the notebook:\n\n* `groupby`: group a dataframe by a column. In this case we will group by the unique client, the `SK_ID_CURR` column\n* `agg`: perform a calculation on the grouped data such as taking the mean of columns. We can either call the function directly (`grouped_df.mean()`) or use the `agg` function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)\n* `merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `SK_ID_CURR` column which will insert `NaN` in any cell for which the client does not have the corresponding statistic\n\nWe also use the (`rename`) function quite a bit specifying the columns to be renamed as a dictionary. This is useful in order to keep track of the new variables we create.\n\nThis might seem like a lot, which is why we'll eventually write a function to do this process for us. Let's take a look at implementing this by hand first. ","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in bureau\nbureau = pd.read_csv('../input/bureau.csv')\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6665d87bd3a157c20fb5a383322aa153005cad2b","trusted":true},"cell_type":"code","source":"# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1adcc4acb891adae8646211db629fb263660c2bb","trusted":true},"cell_type":"code","source":"# Join to the training dataframe\ntrain = pd.read_csv('../input/application_train.csv')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Fill the missing values with 0 \ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53d83da3d8a28541c2dd49be8047e110034c10e6"},"cell_type":"markdown","source":"Scroll all the way to the right to see the new column. ","execution_count":null},{"metadata":{"_uuid":"173b8548125c8a344f67d397d7c24af223af7254"},"cell_type":"markdown","source":"## Assessing Usefulness of New Variable with r value\n\nTo determine if the new variable is useful, we can calculate the Pearson Correlation Coefficient (r-value) between this variable and the target. This measures the strength of a linear relationship between two variables and ranges from -1 (perfectly negatively linear) to +1 (perfectly positively linear). The r-value is not best measure of the \"usefulness\" of a new variable, but it can give a first approximation of whether a variable will be helpful to a machine learning model. The larger the r-value of a variable with respect to the target, the more a change in this variable is likely to affect the value of the target. Therefore, we look for the variables with the greatest absolute value r-value relative to the target.\n\nWe can also visually inspect a relationship with the target using the Kernel Density Estimate (KDE) plot. \n\n用r值评估新变量的有用性\n\n为了确定新变量是否有用，我们可以计算该变量与目标之间的皮尔逊相关系数（r值）。它测量两个变量之间线性关系的强度，范围从-1（完全负线性）到+1（完全正线性）。r值不是衡量一个新变量“有用性”的最佳指标，但它可以给出一个变量是否有助于机器学习模型的第一个近似值。变量相对于目标的r值越大，该变量的变化对目标值的影响就越大。因此，我们寻找相对目标绝对值r值最大的变量。\n\n\n\n我们还可以使用核密度估计（KDE）图直观地检查与目标的关系。","execution_count":null},{"metadata":{"_uuid":"f5bc72f178a44d45399f620ee43483df2482c02f"},"cell_type":"markdown","source":"### Kernel Density Estimate Plots\n\nThe kernel density estimate plot shows the distribution of a single variable (think of it as a smoothed histogram). To see the different in distributions dependent on the value of a categorical variable, we can color the distributions differently according to the category. For example, we can show the kernel density estimate of the `previous_loan_count` colored by whether the `TARGET` = 1 or 0. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan (`TARGET == 1`) and the people who did (`TARGET == 0`). This can serve as an indicator of whether a variable will be 'relevant' to a machine learning model. \n\nWe will put this plotting functionality in a function to re-use for any variable. \n\n核密度估计图显示了单个变量的分布（将其视为平滑直方图）。为了了解与范畴变量值相关的分布的不同，我们可以根据类别对分布进行不同的着色。例如，我们可以显示先前的贷款计数的核密度估计值，由目标值=1还是0来着色。得到的KDE将显示出没有偿还贷款的人（TARGET==1）和还贷的人（TARGET==0）之间变量分布的任何显著差异。这可以作为一个指标，表明一个变量是否与机器学习模型“相关”。\n\n\n\n我们将把这个绘图功能放在一个函数中，以重用任何变量。","execution_count":null},{"metadata":{"_uuid":"4a5fab58ab8327c7f53361b584603d78df96c7a4","trusted":true},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"783e2b23b473f5f61d17ba5b5b64349d3a1d60cf"},"cell_type":"markdown","source":"We can test this function using the `EXT_SOURCE_3` variable which we [found to be one of the most important variables ](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) according to a Random Forest and Gradient Boosting Machine. ","execution_count":null},{"metadata":{"_uuid":"f9e51eb7f02c5612edef3c950ae89da3b8ececf4","trusted":true},"cell_type":"code","source":"kde_target('EXT_SOURCE_3', train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4598cb54ad22b04646683832e2e4c4bc98ff8e9"},"cell_type":"markdown","source":"Now for the new variable we just made, the number of previous loans at other institutions.","execution_count":null},{"metadata":{"_uuid":"bbae3d4e85f106c1e7b457ad71e1549c0e7b1267","trusted":true},"cell_type":"code","source":"kde_target('previous_loan_counts', train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85c4e3f8ccb54bb4dbbd85a414744d301916fd8"},"cell_type":"markdown","source":"From this it's difficult to tell if this variable will be important. The correlation coefficient is extremely weak and there is almost no noticeable difference in the distributions. \n\nLet's move on to make a few more variables from the bureau dataframe. We will take the mean, min, and max of every numeric column in the bureau dataframe.","execution_count":null},{"metadata":{"_uuid":"11f1b0ab2146ec95a50055d2788b5f05c43e0afb"},"cell_type":"markdown","source":"## Aggregating Numeric Columns\n\nTo account for the numeric information in the `bureau` dataframe, we can compute statistics for all the numeric columns. To do so, we `groupby` the client id, `agg` the grouped dataframe, and merge the result back into the training data. The `agg` function will only calculate the values for the numeric columns where the operation is considered valid. We will stick to using `'mean', 'max', 'min', 'sum'` but any function can be passed in here. We can even write our own function and use it in an `agg` call. \n\n\n为了说明 `bureau` 数据框中的数字信息，我们可以计算所有数字列的统计信息。 为此，我们对客户端ID进行分组，对分组的数据框进行 `agg` 转换，然后将结果合并回训练数据中。 `agg` 函数将仅计算认为该操作有效的数字列的值。 我们将坚持使用 `mean`，`max`，`min`，`sum`，但是任何函数都可以在此处传递。 我们甚至可以编写我们自己的函数，并在`agg`调用中使用它。","execution_count":null},{"metadata":{"_uuid":"582885d850fa1316ce869b856b9d886ad542b04b","trusted":true},"cell_type":"code","source":"# Group by the client id, calculate aggregation statistics\nbureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"812f7b1e0d6016768d9276da790b0986181f9fdd"},"cell_type":"markdown","source":"We need to create new names for each of these columns. The following code makes new names by appending the stat to the name. Here we have to deal with the fact that the dataframe has a multi-level index. I find these confusing and hard to work with, so I try to reduce to a single level index as quickly as possible.\n\n我们需要为每个列创建新名称。下面的代码通过在名称后附加stat来生成新名称。这里我们要处理的事实是数据帧有一个多级索引。我发现这些令人费解而且很难处理，所以我试图尽快将其简化为一个级别的索引。","execution_count":null},{"metadata":{"_uuid":"01acafad68d285c875018ae916c002e9dad1a2fa","trusted":true},"cell_type":"code","source":"# List of column names\ncolumns = ['SK_ID_CURR']\n\n# Iterate through the variables names\nfor var in bureau_agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'SK_ID_CURR':\n        \n        # Iterate through the stat names\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('bureau_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f34abd311ffcb34e9feb0786998e478ffd90766","trusted":true},"cell_type":"code","source":"# Assign the list of columns names as the dataframe column names\nbureau_agg.columns = columns\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13c6a8b69a3bdbd1bf2d420d984f78400fc969d3"},"cell_type":"markdown","source":"Now we simply merge with the training data as we did before.\n\n现在，我们像以前一样简单地将训练数据合并。","execution_count":null},{"metadata":{"_uuid":"2da1e6aa5651f4567813896d79d2e087fd5e7749","trusted":true},"cell_type":"code","source":"# Merge with the training data\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"290cf9aaa44d462f0f987ef35bb22b0010bd01a0"},"cell_type":"markdown","source":"### Correlations of Aggregated Values with Target\n\nWe can calculate the correlation of all new values with the target. Again, we can use these as an approximation of the variables which may be important for modeling. \n\n我们可以计算所有新值与目标的相关性。 同样，我们可以将它们用作对建模可能很重要的变量的近似值。","execution_count":null},{"metadata":{"_uuid":"8cf94533bb8df06623f70d0cb01c24156f8af689","trusted":true},"cell_type":"code","source":"# List of new correlations\nnew_corrs = []\n\n# Iterate through the columns \nfor col in columns:\n    # Calculate correlation with the target\n    corr = train['TARGET'].corr(train[col])\n    \n    # Append the list as a tuple\n\n    new_corrs.append((col, corr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"774fe7e4c3e2708de823d3f94f65137b56bc2c2a"},"cell_type":"markdown","source":"In the code below, we sort the correlations by the magnitude (absolute value) using the `sorted` Python function. We also make use of an anonymous `lambda` function, another important Python operation that is good to know. \n\n在下面的代码中，我们使用排序的Python函数按幅度（绝对值）对相关性进行排序。 我们还利用了匿名lambda函数，这是另一个很重要的Python操作，很容易理解。","execution_count":null},{"metadata":{"_uuid":"2802a2568214396e919837353683c63867d38a17","trusted":true},"cell_type":"code","source":"# Sort the correlations by the absolute value\n# Make sure to reverse to put the largest values at the front of list\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"581f41a665ed3b23d8b28a6e3050a11bc8a68dcf"},"cell_type":"markdown","source":"None of the new variables have a significant correlation with the TARGET. We can look at the KDE plot of the highest correlated variable, `bureau_DAYS_CREDIT_mean`, with the target in  in terms of absolute magnitude correlation. \n\n没有任何新变量与TARGET有显着相关性。 我们可以看一下相关性最高的变量KDE图，bureau_DAYS_CREDIT_mean，其中目标是绝对量级相关性。","execution_count":null},{"metadata":{"_uuid":"be155ee8fd05290d11a205b95f4b553b8ade06e3","trusted":true},"cell_type":"code","source":"kde_target('bureau_DAYS_CREDIT_mean', train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ca4b2852b486fa10e1bf31c2a7e61e9de52dd7"},"cell_type":"markdown","source":"The definition of this column is: \"How many days before current application did client apply for Credit Bureau credit\". My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Therefore, a larger negative number indicates the loan was further before the current loan application. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. With a correlation this weak though, it is just as likely to be noise as a signal. \n\n#### The Multiple Comparisons Problem\n\nWhen we have lots of variables, we expect some of them to be correlated just by pure chance, a [problem known as multiple comparisons](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578). We can make hundreds of features, and some will turn out to be corelated with the target simply because of random noise in the data. Then, when our model trains, it may overfit to these variables because it thinks they have a relationship with the target in the training set, but this does not necessarily generalize to the test set. There are many considerations that we have to take into account when making features! \n\n\n此栏的定义是：“客户在当前申请前多少天申请了征信局信贷”。我的解释是，这是在申请家庭信贷贷款之前申请上一笔贷款的天数。因此，一个较大的负数表明贷款是在当前贷款申请之前。我们发现，这个变量的平均值与目标值之间存在极弱的正相关关系，这意味着过去申请贷款的客户更有可能通过家庭信贷偿还贷款。虽然相关性如此微弱，但它很可能是一个信号噪声。\n\n\n\n多重比较问题\n\n当我们有很多变量时，我们期望它们中的一些仅仅是纯粹的偶然关联，这个问题被称为多重比较。我们可以做上百个特征，有些特征会因为数据中的随机噪声而被证明与目标相关。然后，当我们的模型训练时，它可能会对这些变量过度拟合，因为它认为这些变量与训练集中的目标有关系，但这并不一定推广到测试集。在制作功能时，我们需要考虑很多因素！","execution_count":null},{"metadata":{"_uuid":"645ab51cb64cd20f122d61e17ef8ad1ed78443cc"},"cell_type":"markdown","source":"## Function for Numeric Aggregations\n\nLet's encapsulate all of the previous work into a function. This will allow us to compute aggregate stats for numeric columns across any dataframe. We will re-use this function when we want to apply the same operations for other dataframes.\n\n\n数值聚合函数\n\n让我们将所有先前的工作封装到一个函数中。 这将使我们能够计算跨任何数据框的数字列的汇总统计信息。 当我们想对其他数据框应用相同的操作时，我们将重用此函数。","execution_count":null},{"metadata":{"_uuid":"8203852b47164d7800f69e572dda2cbe83c50976","trusted":true},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2784423d6e900b66f108cb2e6a425d63eb28743e","trusted":true},"cell_type":"code","source":"bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171660155dd332d9df85f61fdd7004a9f2593e59"},"cell_type":"markdown","source":"To make sure the function worked as intended, we should compare with the aggregated dataframe we constructed by hand. \n\n为了确保功能按预期工作，我们应该与手工构建的聚合数据框进行比较。","execution_count":null},{"metadata":{"_uuid":"57dab1b1b23d0bd0167c9367688b56b43066a721","trusted":true},"cell_type":"code","source":"bureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af5a3d2ebc6850707c0bb021bc08df50b98d008b"},"cell_type":"markdown","source":"If we go through and inspect the values, we do find that they are equivalent. We will be able to reuse this function for calculating numeric stats for other dataframes. Using functions allows for consistent results and decreases the amount of work we have to do in the future! \n\n如果我们仔细检查这些值，就会发现它们是等效的。 我们将能够重用此函数来计算其他数据框的数值统计。 使用功能可确保获得一致的结果并减少我们将来要做的工作！\n\n### Correlation Function\n\nBefore we move on, we can also make the code to calculate correlations with the target into a function.\n\n在继续之前，我们还可以编写代码以将与目标的相关性计算为函数。\n\n\n","execution_count":null},{"metadata":{"_uuid":"f55cdb9e28c9fcdb16b7922ce90c99ddda590e67","trusted":true},"cell_type":"code","source":"# Function to calculate correlations with the target for a dataframe\ndef target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a86ad170924ebf66122d24083679d2a061a080ff"},"cell_type":"markdown","source":"## Categorical Variables\n\nNow we move from the numeric columns to the categorical columns. These are discrete string variables, so we cannot just calculate statistics such as mean \nand max which only work with numeric variables. Instead, we will rely on calculating value counts of each category within each categorical variable. As an example, if we have the following dataframe:\n\n现在，我们从数字列移动到分类列。 这些是离散的字符串变量，因此我们不能仅计算仅适用于数字变量的统计数据（例如均值和最大值）。 相反，我们将依靠计算每个类别变量中每个类别的值计数。 例如，如果我们具有以下数据框：\n\n\n| SK_ID_CURR | Loan type |\n|------------|-----------|\n| 1          | home      |\n| 1          | home      |\n| 1          | home      |\n| 1          | credit    |\n| 2          | credit    |\n| 3          | credit    |\n| 3          | cash      |\n| 3          | cash      |\n| 4          | credit    |\n| 4          | home      |\n| 4          | home      |\n\nwe will use this information counting the number of loans in each category for each client. \n\n我们将使用此信息计算每个客户在每个类别中的贷款数量\n\n| SK_ID_CURR | credit count | cash count | home count | total count |\n|------------|--------------|------------|------------|-------------|\n| 1          | 1            | 0          | 3          | 4           |\n| 2          | 1            | 0          | 0          | 1           |\n| 3          | 1            | 2          | 0          | 3           |\n| 4          | 1            | 0          | 2          | 3           |\n\n\nThen we can normalize these value counts by the total number of occurences of that categorical variable for that observation (meaning that the normalized counts must sum to 1.0 for each observation).\n\n然后，我们可以通过该观测值的分类变量的出现总数来归一化这些值计数（这意味着每个观测值的归一化计数必须总和为1.0）。\n\n| SK_ID_CURR | credit count | cash count | home count | total count | credit count norm | cash count norm | home count norm |\n|------------|--------------|------------|------------|-------------|-------------------|-----------------|-----------------|\n| 1          | 1            | 0          | 3          | 4           | 0.25              | 0               | 0.75            |\n| 2          | 1            | 0          | 0          | 1           | 1.00              | 0               | 0               |\n| 3          | 1            | 2          | 0          | 3           | 0.33              | 0.66            | 0               |\n| 4          | 1            | 0          | 2          | 3           | 0.33              | 0               | 0.66            |\n\nHopefully, encoding the categorical variables this way will allow us to capture the information they contain. If anyone has a better idea for this process, please let me know in the comments!\nWe will now go through this process step-by-step. At the end, we will wrap up all the code into one function to be re-used for many dataframes.\n\n希望以这种方式对分类变量进行编码将使我们能够捕获它们包含的信息。 如果有人对此过程有更好的主意，请在评论中让我知道！ 现在，我们将逐步完成此过程。 最后，我们将所有代码包装到一个函数中，以用于许多数据帧。","execution_count":null},{"metadata":{"_uuid":"3a8c1a8a74741d727a17ec0e83c237abb931e89e"},"cell_type":"markdown","source":"First we one-hot encode a dataframe with only the categorical columns (`dtype == 'object'`).\n\n首先，我们仅对类别列（dtype =='object'）进行数据编码。","execution_count":null},{"metadata":{"trusted":true,"_uuid":"cd9fee51331a8181b29867d04b568336aa2bd445"},"cell_type":"code","source":"categorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"839ae477431e4131cfecc3bc0b2046ee964c6623"},"cell_type":"code","source":"categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed965c98dee32b0b9194a97c0babd1ade9896e52"},"cell_type":"markdown","source":"The `sum` columns represent the count of that category for the associated client and the `mean` represents the normalized count. One-hot encoding makes the process of calculating these figures very easy!\n\nWe can use a similar function as before to rename the columns. Again, we have to deal with the multi-level index for the columns. We iterate through the first level (level 0) which is the name of the categorical variable appended with the value of the category (from one-hot encoding). Then we iterate  stats we calculated for each client. We will rename the column with the level 0 name appended with the stat. As an example, the column with `CREDIT_ACTIVE_Active` as level 0 and `sum` as level 1 will become `CREDIT_ACTIVE_Active_count`. \n\nsum列表示关联客户的该类别的计数，平均值表示标准化计数。 一键编码使计算这些数字的过程变得非常容易！\n\n我们可以使用与以前类似的功能来重命名列。 同样，我们必须处理列的多级索引。 我们迭代第一个级别（级别0），该级别是类别变量的名称，后面附加有类别的值（来自单编码）。 然后，我们迭代为每个客户计算的统计信息。 我们将重命名带有stat的0级名称的列。 例如，将CREDIT_ACTIVE_Active设置为0级并将sum设置为1级的列将变为CREDIT_ACTIVE_Active_count。","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d0348b245ddb0e1f7bfa1e72b5c2e5b04416f66b"},"cell_type":"code","source":"categorical_grouped.columns.levels[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbfa21f67f3fa910bdb052f482c6cbf684a2e474"},"cell_type":"code","source":"categorical_grouped.columns.levels[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for stat in ['count', 'count_norm']:\n    # Make a new column name for the variable and stat\n    print('%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41dccd044bedbf169b761daeff0e5d7b177d08ef"},"cell_type":"code","source":"group_var = 'SK_ID_CURR'\n\n# Need to create new column names\ncolumns = []\n\n# Iterate through the variables names\nfor var in categorical_grouped.columns.levels[0]:\n    # Skip the grouping variable\n    if var != group_var:\n        # Iterate through the stat names\n        for stat in ['count', 'count_norm']:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (var, stat))\n\n#  Rename the columns\ncategorical_grouped.columns = columns\n\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ebc592c38c6d7664c7c57d3884ea80dfeabf760"},"cell_type":"markdown","source":"The sum column records the counts and the mean column records the normalized count. \n\nWe can merge this dataframe into the training data.\n\n“总和”列记录计数，“平均值”列记录归一化计数。\n\n我们可以将此数据框合并到训练数据中。","execution_count":null},{"metadata":{"trusted":true,"_uuid":"69e0695167a8e793c1a1d3934f8a8e510343606d"},"cell_type":"code","source":"train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"072b94cbbc2dba8df21d777e46bc693d731fd512"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ba0118d6544d6d4fe436901ee9dc8834ae1b0e"},"cell_type":"code","source":"train.iloc[:10, 123:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a30770ca960821a6c915f6acd54161a37bffb432"},"cell_type":"markdown","source":"### Function to Handle Categorical Variables\n\nTo make the code more efficient, we can now write a function to handle the categorical variables for us. This will take the same form as the `agg_numeric` function in that it accepts a dataframe and a grouping variable. Then it will calculate the counts and normalized counts of each category for all categorical variables in the dataframe.\n\n处理分类变量的功能\n\n为了使代码更有效，我们现在可以编写一个函数来为我们处理分类变量。 这将采用与agg_numeric函数相同的形式，因为它接受一个数据帧和一个分组变量。 然后，它将为数据框中的所有类别变量计算每个类别的计数和归一化计数。","execution_count":null},{"metadata":{"_uuid":"ddc7decf9497eaa536255d601f76c3a6259fca6b","trusted":true},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']: # 注意这里的归一化列之所以有值是因为将mean列的数据改为了归一化的值\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0f1629b07ec6e75e98da459f22611d275fcb282","trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cecd442561b3a8405721875878bcbb90c94f2517"},"cell_type":"markdown","source":"### Applying Operations to another dataframe\n\nWe will now turn to the bureau balance dataframe. This dataframe has monthly information about each client's previous loan(s) with other financial institutions. Instead of grouping this dataframe by the `SK_ID_CURR` which is the client id, we will first group the dataframe by the `SK_ID_BUREAU` which is the id of the previous loan. This will give us one row of the dataframe for each loan. Then, we can group by the `SK_ID_CURR` and calculate the aggregations across the loans of each client. The final result will be a dataframe with one row for each client, with stats calculated for their loans.\n\n### 将操作应用于另一个数据框\n\n现在，我们将转到 bureau  balance dataframe。 该 dataframe 具有有关每个客户以前与其他金融机构的贷款的每月信息。 与其按“ SK_ID_CURR”（即客户ID）对数据框进行分组，我们首先将按“ SK_ID_BUREAU”（即先前贷款的ID）对数据框进行分组。 这将为我们提供每笔贷款数据框的一行。 然后，我们可以对“ SK_ID_CURR”进行分组，并计算每个客户贷款的总和。 最终结果将是一个数据框，其中每个客户一行，并为他们的贷款计算统计数据。","execution_count":null},{"metadata":{"_uuid":"b49d1c1553eb3c0a8887d967fe9c981b015a7e40","trusted":true},"cell_type":"code","source":"# Read in bureau balance\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"815ea915661653e3322e5cbf0f60614754411526"},"cell_type":"markdown","source":"First, we can calculate the value counts of each status for each loan. Fortunately, we already have a function that does this for us! ","execution_count":null},{"metadata":{"_uuid":"891d69c6cf3634d2fab71b0c8cc1eb212f433083","trusted":true},"cell_type":"code","source":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b1b1623a426efd45f7488636ffffc6b54c68d8e"},"cell_type":"markdown","source":"Now we can handle the one numeric column. The `MONTHS_BALANCE` column has the \"months of balance relative to application date.\" This might not necessarily be that important as a numeric variable, and in future work we might want to consider this as a time variable. For now, we can just calculate the same aggregation statistics as previously. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da531864649dc2b8dab18c31019fc36d6ec6469d","trusted":true},"cell_type":"code","source":"# Calculate value count statistics for each `SK_ID_CURR` \nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50b714b2165463138c29715ad68919bcc47aac2d"},"cell_type":"markdown","source":"The above dataframes have the calculations done on each _loan_. Now we need to aggregate these for each _client_. We can do this by merging the dataframes together first and then since all the variables are numeric, we just need to aggregate the statistics again, this time grouping by the `SK_ID_CURR`. \n\n以上数据框具有对每笔贷款的计算。 现在，我们需要为每个客户汇总这些信息。 为此，我们可以先将数据帧合并在一起，然后再合并所有变量，因为它们都是数字变量，我们只需要再次汇总统计信息即可，这一次是按SK_ID_CURR进行分组。","execution_count":null},{"metadata":{"_uuid":"dbd657d3cbe3cf9948ba8d7543f6653e61025bc6","trusted":true},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae22ded2231b37858d9d1d0c3c5b0eb72202c6ab","trusted":true},"cell_type":"code","source":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb037a66b921c2dbf837e8d6ddb5179be51e8446"},"cell_type":"markdown","source":"To recap, for the `bureau_balance` dataframe we:\n\n1. Calculated numeric stats grouping by each loan\n2. Made value counts of each categorical variable grouping by loan\n3. Merged the stats and the value counts on the loans\n4. Calculated numeric stats for the resulting dataframe grouping by the client id\n\nThe final resulting dataframe has one row for each client, with statistics calculated for all of their loans with monthly balance information. \n\nSome of these variables are a little confusing, so let's try to explain a few:\n\n* `client_bureau_balance_MONTHS_BALANCE_mean_mean`: For each loan calculate the mean value of `MONTHS_BALANCE`. Then for each client, calculate the mean of this value for all of their loans. \n* `client_bureau_balance_STATUS_X_count_norm_sum`: For each loan, calculate the number of occurences of `STATUS` == X divided by the number of total `STATUS` values for the loan. Then, for each client, add up the values for each loan. \n\n\n回顾一下，对于“ bureau_balance”数据帧，我们：\n\n* 1.按每笔贷款计算的统计数字分组\n* 2.按贷款分类的每个分类变量的产值计数\n* 3.合并贷款的统计信息和价值计数\n* 4.计算出的数据统计值，用于按客户端ID分组的结果数据框\n\n最终的最终数据帧为每个客户提供一行，并为他们的所有贷款计算出带有每月余额信息的统计信息。\n\n其中一些变量有些令人困惑，因此让我们尝试解释一些问题：\n\n* `client_bureau_balance_MONTHS_BALANCE_mean_mean`：对于每笔贷款，请计算`MONTHS_BALANCE`的平均值。 然后，为每个客户计算所有贷款的该价值的平均值。\n\n* `client_bureau_balance_STATUS_X_count_norm_sum`：对于每笔贷款，计算“ STATUS”的发生次数== X除以该贷款的“ STATUS”总值的数量。 然后，对于每个客户，将每个贷款的值相加。","execution_count":null},{"metadata":{"_uuid":"e7c7be26bc22cacc8168f907d01c2c259ad6c867"},"cell_type":"markdown","source":"We will hold off on calculating the correlations until we have all the variables together in one dataframe. ","execution_count":null},{"metadata":{"_uuid":"90aa8e57fbff00777a1e62b060c2c06cf7845381"},"cell_type":"markdown","source":"# Putting the Functions Together\n\nWe now have all the pieces in place to take the information from the previous loans at other institutions and the monthly payments information about these loans and put them into the main training dataframe. Let's do a reset of all the variables and then use the functions we built to do this from the ground up. This demonstrate the benefit of using functions for repeatable workflows! ","execution_count":null},{"metadata":{"_uuid":"26848bd0252a31559e254c454b905305701ee522","trusted":true},"cell_type":"code","source":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc574b9486fb40f72146e6481b341931dd180e3d","trusted":true},"cell_type":"code","source":"# Read in new copies of all the dataframes\ntrain = pd.read_csv('../input/application_train.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feba956fa449260cc71505cb9156887a62ca8337"},"cell_type":"markdown","source":"### Counts of Bureau Dataframe","execution_count":null},{"metadata":{"_uuid":"491f45986682beb4ee2bd62c32c622830b3b0e76","trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c80230ee098fa39f4446b7161f105cd18965bc"},"cell_type":"markdown","source":"### Aggregated Stats of Bureau Dataframe","execution_count":null},{"metadata":{"_uuid":"5f9d2491e244da4811f78f0340fd9d42127e9076","trusted":true},"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ce2211679505e646121cc0b2f8b217ab9718ba6"},"cell_type":"markdown","source":"### Value counts of Bureau Balance dataframe by loan","execution_count":null},{"metadata":{"_uuid":"0e55cc0b41e66e77dfd19ac1b808edc4581a1206","trusted":true},"cell_type":"code","source":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c2378a4fc130bc7de954c454d50e26adda6f9f6"},"cell_type":"markdown","source":"### Aggregated stats of Bureau Balance dataframe by loan","execution_count":null},{"metadata":{"_uuid":"eb25294df7f998058e15e6ae24438c772d4ae86c","trusted":true},"cell_type":"code","source":"bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7343f605c4bc38c0c3af1b53bc258699e480dcf"},"cell_type":"markdown","source":"### Aggregated Stats of Bureau Balance by Client","execution_count":null},{"metadata":{"_uuid":"9315370e38f25339af200a45809387f211464ce9","trusted":true},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f8f616b645ba974517be24adb30b4af4cf1afe3"},"cell_type":"markdown","source":"## Insert Computed Features into Training Data","execution_count":null},{"metadata":{"_uuid":"f61f183707d7ca8d363a344b073dd9122acfdcc5","trusted":true},"cell_type":"code","source":"original_features = list(train.columns)\nprint('Original Number of Features: ', len(original_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02f4310f7485d299c1588805ce24f3360afa6f42","trusted":true},"cell_type":"code","source":"# Merge with the value counts of bureau\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f672226eee3059311d476c0eb68946988f9f5712","trusted":true},"cell_type":"code","source":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e16d0c10cf2a0cc778db576a4ad233960c70a028"},"cell_type":"markdown","source":"# Feature Engineering Outcomes\n\nAfter all that work, now we want to take a look at the variables we have created. We can look at the percentage of missing values, the correlations of variables with the target, and also the correlation of variables with the other variables. The correlations between variables can show if we have collinear varibles, that is, variables that are highly correlated with one another. Often, we want to remove one in a pair of collinear variables because having both variables would be redundant. We can also use the percentage of missing values to remove features with a substantial majority of values that are not present. __Feature selection__ will be an important focus going forward, because reducing the number of features can help the model learn during training and also generalize better to the testing data. The \"curse of dimensionality\" is the name given to the issues caused by having too many features (too high of a dimension). As the number of variables increases, the number of datapoints needed to learn the relationship between these variables and the target value increases exponentially. \n\nFeature selection is the process of removing variables to help our model to learn and generalize better to the testing set. The objective is to remove useless/redundant variables while preserving those that are useful. There are a number of tools we can use for this process, but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. Later we can look at using the feature importances returned from models such as the `Gradient Boosting Machine` or `Random Forest` to perform feature selection.\n\n\n完成所有这些工作之后，现在我们想看看我们创建的变量。我们可以查看缺失值的百分比，变量与目标的相关性以及变量与其他变量的相关性。变量之间的相关性可以表明我们是否具有共线变量，即彼此之间高度相关的变量。通常，我们希望删除一对共线变量中的一个，因为同时拥有两个变量将是多余的。我们还可以使用缺失值的百分比来删除大部分不存在的要素。特征选择将是未来的重要重点，因为减少特征数量可以帮助模型在训练过程中学习，并且可以更好地推广到测试数据。 “维数诅咒”是因特征过多（维数过高）而引起的问题的名称。随着变量数量的增加，学习这些变量与目标值之间的关系所需的数据点数量呈指数增长。\n\n特征选择是删除变量以帮助我们的模型学习并更好地推广到测试集的过程。目的是在保留有用的变量的同时删除无用/冗余的变量。我们可以使用许多工具来完成此过程，但是在本笔记本中，我们将坚持删除缺失值和变量之间具有很高相关性的列，这些列中的缺失值和变量百分比很高。稍后，我们可以看一下使用诸如Gradient Boosting Machine或Random Forest之类的模型返回的特征重要性来进行特征选择。\n","execution_count":null},{"metadata":{"_uuid":"ab576d2c245270ca843f825afab02583c091897d"},"cell_type":"markdown","source":"## Missing Values\n\nAn important consideration is the missing values in the dataframe. Columns with too many missing values might have to be dropped. ","execution_count":null},{"metadata":{"_uuid":"eef874f5edd28e7449e198264987b93566a5227a","trusted":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dba1e683347bbd9b07a162675a4f79e08c28db0a","trusted":true},"cell_type":"code","source":"missing_train = missing_values_table(train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0defbc704277c67350cf2064b511800a3cdbad2"},"cell_type":"markdown","source":"We see there are a number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, and the best course of action depends on the problem. Here, to reduce the number of features, we will remove any columns in either the training or the testing data that have greater than 90% missing values.\n\n我们看到许多列的缺失值比例很高。 没有消除缺失值的公认阈值，最佳的解决方案取决于问题。 在这里，为了减少功能部件的数量，我们将删除训练或测试数据中缺失值大于90％的所有列。","execution_count":null},{"metadata":{"_uuid":"5739842767f1a58e9c33a6683d5b0f32c97b69b3","trusted":true},"cell_type":"code","source":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"055b96cb462b6876d6ae7f0e0c348426ddfc5cca"},"cell_type":"markdown","source":"Before we remove the missing values, we will find the missing value percentages in the testing data. We'll then remove any columns with greater than 90% missing values in either the training or testing data.\nLet's now read in the testing data, perform the same operations, and look at the missing values in the testing data. We already have calculated all the counts and aggregation statistics, so we only need to merge the testing data with the appropriate data. \n\n在删除缺失值之前，我们将在测试数据中找到缺失值百分比。 然后，我们将删除训练或测试数据中所有缺失值大于90％的列。 现在，让我们读入测试数据，执行相同的操作，并查看测试数据中缺少的值。 我们已经计算了所有计数和聚合统计信息，因此我们只需要将测试数据与适当的数据合并即可。\n\n","execution_count":null},{"metadata":{"_uuid":"dc3921576a8fb644b36f5119f169b88d30a51c01"},"cell_type":"markdown","source":"## Calculate Information for Testing Data","execution_count":null},{"metadata":{"_uuid":"48be68ba3403d8e3f208b436219003b1f6097fad","trusted":true},"cell_type":"code","source":"# Read in the test dataframe\ntest = pd.read_csv('../input/application_test.csv')\n\n# Merge with the value counts of bureau\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee6595328bd2eab6a89b542475500aaaa4b3947","trusted":true},"cell_type":"code","source":"print('Shape of Testing Data: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"518da486b84f28f7ca9c1b366fdb2719f0e19f38"},"cell_type":"markdown","source":"We need to align the testing and training dataframes, which means matching up the columns so they have the exact same columns. This shouldn't be an issue here, but when we one-hot encode variables, we need to align the dataframes to make sure they have the same columns.\n\n我们需要对齐测试和培训数据框，这意味着要匹配各列，以便它们具有完全相同的列。 在这里这不应该成为问题，但是当我们热编码变量时，我们需要对齐数据框以确保它们具有相同的列。","execution_count":null},{"metadata":{"_uuid":"39d6e6a2b8af1bb15df0f98ff5eb601c2e828099","trusted":true},"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3d3fcc7cf9a68208a6d6303565883cd58e7c531","trusted":true},"cell_type":"code","source":"print('Training Data Shape: ', train.shape)\nprint('Testing Data Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77cd0cbc96158d555ca156b93906497364cbdabd"},"cell_type":"markdown","source":"The dataframes now have the same columns (with the exception of the `TARGET` column in the training data). This means we can use them in a machine learning model which needs to see the same columns in both the training and testing dataframes.\n\nLet's now look at the percentage of missing values in the testing data so we can figure out the columns that should be dropped.\n\n\n现在，数据框具有相同的列（训练数据中的TARGET列除外）。 这意味着我们可以在机器学习模型中使用它们，该模型需要在训练和测试数据帧中看到相同的列。\n\n现在，让我们看一下测试数据中缺失值的百分比，以便我们找出应该删除的列。","execution_count":null},{"metadata":{"_uuid":"ff8e3932b44097ae679ea7bb9b0464a077e07727","trusted":true},"cell_type":"code","source":"missing_test = missing_values_table(test)\nmissing_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ecf51b6455fdef32ea7b50cdaf05eed211b15e7","trusted":true},"cell_type":"code","source":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\nlen(missing_test_vars)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caa333e30cf19898f4b902d64fbd790659c8f035","trusted":true},"cell_type":"code","source":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ae7c69dc4112159c5e1356b06821ba169ac595d","trusted":true},"cell_type":"code","source":"# Drop the missing columns\ntrain = train.drop(columns = missing_columns)\ntest = test.drop(columns = missing_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba618f25fe2fb25681228d518cc56f88572cf7fd"},"cell_type":"markdown","source":"We ended up removing no columns in this round because there are no columns with more than 90% missing values. We might have to apply another feature selection method to reduce the dimensionality. \n\n由于没有丢失值超过90％的列，我们最终没有删除任何列。 我们可能必须应用另一种特征选择方法以减小维数。","execution_count":null},{"metadata":{"_uuid":"91197da37fcd84538520f03b50e0b060a19cc71d"},"cell_type":"markdown","source":"At this point we will save both the training and testing data. I encourage anyone to try different percentages for dropping the missing columns and compare the outcomes. \n\n此时，我们将保存训练和测试数据。 我鼓励任何人尝试不同的百分比来删除缺少的列并比较结果。","execution_count":null},{"metadata":{"_uuid":"a7d15638e419b84bf164b20e5195db37885e3e0d","trusted":true},"cell_type":"code","source":"# train.to_csv('train_bureau_raw.csv', index = False)\n# test.to_csv('test_bureau_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9425c558df0c882cffe4fc07da41394a6fb3b422"},"cell_type":"markdown","source":"## Correlations\n\nFirst let's look at the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data (from `application`). ","execution_count":null},{"metadata":{"_uuid":"87c47e087eaacb846572ffdd58c22ca1f0ce0010","trusted":true},"cell_type":"code","source":"# Calculate all correlations in dataframe\ncorrs = train.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75c757ae81ff84d9e3453ca7c74ce59326e24b84","trusted":true},"cell_type":"code","source":"corrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6774bdef9c97bc58191101414a0a067561f12160","trusted":true},"cell_type":"code","source":"# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d98ec8d33f097aa3702933cfb64d82bfbd62232"},"cell_type":"markdown","source":"The highest correlated variable with the target (other than the `TARGET` which of course has a correlation of 1), is a variable we created. However, just because the variable is correlated does not mean that it will be useful, and we have to remember that if we generate hundreds of new variables, some are going to be correlated with the target simply because of random noise. \n\nViewing the correlations skeptically, it does appear that several of the newly created variables may be useful. To assess the \"usefulness\" of variables, we will look at the feature importances returned by the model. For curiousity's sake (and because we already wrote the function) we can make a kde plot of two of the newly created variables.\n\n与目标相关性最高的变量（除了TARGET当然具有1的相关性）是我们创建的变量。 但是，仅仅因为变量是相关的并不意味着它会有用，我们必须记住，如果我们生成数百个新变量，则仅仅由于随机噪声，某些变量将与目标相关。\n\n怀疑地查看相关性，似乎确实有一些新创建的变量可能有用。 为了评估变量的“有用性”，我们将研究模型返回的功能重要性。 出于好奇的缘故（并且因为我们已经编写了函数），我们可以对两个新创建的变量进行kde图绘制。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c92a7810a7e0e6fd5d92e1be50749e427fae7c11","trusted":true},"cell_type":"code","source":"# kde_target(var_name='client_bureau_balance_counts_mean', df=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8c47b227c6c4130ea9d3454a789b82c422d20a5"},"cell_type":"markdown","source":"This variable represents the average number of monthly records per loan for each client. For example, if a client had three previous loans with 3, 4, and 5 records in the monthly data, the value of this variable for them would be 4. Based on the distribution, clients with a greater number of average monthly records per loan were more likely to repay their loans with Home Credit. Let's not read too much into this value, but it could indicate that clients who have had more previous credit history are generally more likely to repay a loan.\n\n此变量表示每个客户每笔贷款的平均每月记录数。 例如，如果一个客户以前有3笔贷款，每月数据中分别有3、4和5条记录，则此变量的值将为4。根据分布，每笔贷款平均每月记录数更多的客户 更有可能通过房屋信贷来偿还贷款。 我们不要过多地看这个值，但是它可能表明以前拥有更多信用记录的客户通常更有可能偿还贷款。","execution_count":null},{"metadata":{"_uuid":"17e1cb7c492b1c122556ac16472fbbdee5a1030a","trusted":true},"cell_type":"code","source":"kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd71a301ca0fe799b5f36ddd07cf3224c03e1cd9"},"cell_type":"markdown","source":"Well this distribution is all over the place. This variable represents the number of previous loans with a `CREDIT_ACTIVE` value of `Active` divided by the total number of previous loans for a client. The correlation here is so weak that I do not think we should draw any conclusions! \n\n好吧，这种分布遍布各地。 此变量表示CREDIT_ACTIVE值为Active的先前贷款数除以客户先前的贷款总数。 这里的相关性太弱了，我不认为我们应该得出任何结论！","execution_count":null},{"metadata":{"_uuid":"e6ed8ce92d28415771fe4e4cc872117a18357181"},"cell_type":"markdown","source":"### Collinear Variables\n\nWe can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. \n\nLet's look for any variables that have a greather than 0.8 correlation with other variables.\n\n共线变量\n\n我们不仅可以计算变量与目标的相关性，还可以计算每个变量与其他每个变量的相关性。 这将使我们看到是否存在应该从数据中删除的高度共线变量。\n\n让我们寻找与其他变量的相关性大于0.8的任何变量。","execution_count":null},{"metadata":{"_uuid":"7330e7311864a70afa75b7e7c33ad4b3d77df040","trusted":true},"cell_type":"code","source":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dcfa3892d84efa2e0db448b9542c9effad01e85"},"cell_type":"markdown","source":"For each of these pairs of highly correlated variables, we only want to remove one of the variables. The following code creates a set of variables to remove by only adding one of each pair. \n\n对于每对高度相关的变量，我们只想删除其中一个变量。 以下代码创建了一组变量，只需将每个变量对中的一个相加即可删除。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"above_threshold_vars","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0134543670e895436769c64a58bbdee7eb7a255f","trusted":true},"cell_type":"code","source":"# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\ncols_to_remove_pair = list(set(cols_to_remove))\n\nprint('Number of columns to remove: ', len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove == cols_to_remove_pair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cols_to_remove_pair)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be37ec11c061291e143c01102703103c0915ffcb"},"cell_type":"markdown","source":"We can remove these columns from both the training and the testing datasets. We will have to compare performance after removing these variables with performance keeping these variables (the raw csv files we saved earlier).\n\n我们可以从训练和测试数据集中删除这些列。 在删除这些变量之后，我们将不得不比较性能与保留这些变量的性能（我们之前保存的原始csv文件）。","execution_count":null},{"metadata":{"_uuid":"0fedcbce9bce99809bf7d4559b6ad93e8ecca8ea","trusted":true},"cell_type":"code","source":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f4443a62f6e8987c0be2e3162589444ecd7d275","trusted":true},"cell_type":"code","source":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be48572e88f64f00b4221da7782ac834444c96b0"},"cell_type":"markdown","source":"# Modeling \n\nTo actually test the performance of these new datasets, we will try using them for machine learning! Here we will use a function I developed in another notebook to compare the features (the raw version with the highly correlated variables removed). We can run this kind of like an experiment, and the control will be the performance of just the `application` data in this function when submitted to the competition. I've already recorded that performance, \nso we can list out our control and our two test conditions:\n\n__For all datasets, use the model shown below (with the exact hyperparameters).__\n\n* control: only the data in the `application` files. \n* test one: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files\n* test two: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files with highly correlated variables removed.\n\n\n\n为了实际测试这些新数据集的性能，我们将尝试将它们用于机器学习！ 在这里，我们将使用我在另一个笔记本中开发的功能来比较功能（原始版本中已删除高度相关变量）。 我们可以像实验一样进行这种操作，并且控件将仅在提交给比赛时使用此函数中的应用程序数据来执行性能。 我已经记录了该性能，因此我们可以列出控件和两个测试条件：\n\n对于所有数据集，请使用下面显示的模型（带有确切的超参数）。\n\n* 控制：仅应用程序文件中的数据。\n* 测试之一：应用程序文件中的数据与从局记录的所有数据和Bureau_balance文件\n* 测试二：应用程序文件中的数据已删除了从Bureau记录的所有数据，并且已删除了高度相关变量的Bureau_balance文件。","execution_count":null},{"metadata":{"_uuid":"58aac4a109bde812ca8b75b1c9271550dcf2d397","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae2c8a505885482595f027e6139b55966a88861e","trusted":true},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # 提取特征名称\n    feature_names = list(features.columns)\n    \n    # 转换为np数组\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b39fe9584e0308b7e940c11e6de04b262d123c7","trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52cc81edc0b29905d2325f201f7902e96c321e32"},"cell_type":"markdown","source":"### Control\n\nThe first step in any experiment is establishing a control. For this we will use the function defined above (that implements a Gradient Boosting Machine model) and the single main data source (`application`). \n\n任何实验的第一步都是建立对照。 为此，我们将使用上面定义的函数（实现Gradient Boosting Machine模型）和单个主数据源（应用程序）。","execution_count":null},{"metadata":{"_uuid":"7b9f0e5faa3de9403f87032fe3fc339cb732ad17","trusted":true},"cell_type":"code","source":"train_control = pd.read_csv('../input/application_train.csv')\ntest_control = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74c09643eee73b19d80acfca60dcd33df8747364"},"cell_type":"markdown","source":"Fortunately, once we have taken the time to write a function, using it is simple (if there's a central theme in this notebook, it's use functions to make things simpler and reproducible!). The function above returns a `submission` dataframe we can upload to the competition, a `fi` dataframe of feature importances, and a `metrics` dataframe with validation and test performance. \n\n幸运的是，一旦我们花了时间编写一个函数，它的使用就很简单（如果此笔记本中有一个中心主题，则可以使用函数使事情变得更简单和可再现！）。 上面的函数返回一个可以上传到竞赛中的提交数据框，一个具有重要功能的fi数据框以及一个具有验证和测试性能的指标数据框。","execution_count":null},{"metadata":{"_uuid":"3711f26b43863952e4edba7c3fc33f877ed499b2","trusted":true},"cell_type":"code","source":"submission, fi, metrics = model(train_control, test_control)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed9ad5478719de194aeabec9b151f21effb99f5","trusted":true},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f3382baf72ccfc101660b2f062fee707e7cb041"},"cell_type":"markdown","source":"The control slightly overfits because the training score is higher than the validation score. We can address this in later notebooks when we look at regularization (we already perform some regularization in this model by using `reg_lambda` and `reg_alpha` as well as early stopping). \n\nWe can visualize the feature importance with another function, `plot_feature_importances`. The feature importances may be useful when it's time for feature selection. \n\n由于训练分数高于验证分数，因此控件略为过拟合。 我们可以在以后的笔记本中查看正则化时解决此问题（通过使用reg_lambda和reg_alpha以及提前停止，我们已经在此模型中执行了一些正则化）。\n\n我们可以使用另一个函数plot_feature_importances可视化特征重要性。 在选择功能时，功能重要性可能会很有用。","execution_count":null},{"metadata":{"_uuid":"c09f30bd4fb6e12b55d5a288fab358ffd1fe6f4d","trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fe65cc25940a40c9966101acf9ea8f8477d7b77","trusted":true},"cell_type":"code","source":"submission.to_csv('control.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e502ef191ea2201b05f85cece2202931744fa539"},"cell_type":"markdown","source":"__The control scores 0.745 when submitted to the competition.__","execution_count":null},{"metadata":{"_uuid":"392da17505a7feb3320242ff9785ac81346ee0b7"},"cell_type":"markdown","source":"### Test One\n\nLet's conduct the first test. We will just need to pass in the data to the function, which does most of the work for us.","execution_count":null},{"metadata":{"_uuid":"640bceff24f8c53aed68568c6ee1f70575adab69","trusted":true,"collapsed":true},"cell_type":"code","source":"submission_raw, fi_raw, metrics_raw = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b404e436782d6de379bfa4756d58bed0175396f8","trusted":true,"collapsed":true},"cell_type":"code","source":"metrics_raw","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fcb737753ca178e32afda3fe29bf4c29262c379"},"cell_type":"markdown","source":"Based on these numbers, the engineered features perform better than the control case. However, we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. ","execution_count":null},{"metadata":{"_uuid":"3313e261d947645ca0e72f824de45827a43de9c1","trusted":true,"collapsed":true},"cell_type":"code","source":"fi_raw_sorted = plot_feature_importances(fi_raw)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bf21f18e6430cea006cdd6b0139c52cb760d6ee"},"cell_type":"markdown","source":"Examining the feature improtances, it looks as if a few of the feature we constructed are among the most important. Let's find the percentage of the top 100 most important features that we made in this notebook. However, rather than just compare to the original features, we need to compare to the _one-hot encoded_ original features. These are already recorded for us in `fi` (from the original data). ","execution_count":null},{"metadata":{"_uuid":"b5abb37ffe89b767a01f494801c4db5bc3abc6db","trusted":true,"collapsed":true},"cell_type":"code","source":"top_100 = list(fi_raw_sorted['feature'])[:100]\nnew_features = [x for x in top_100 if x not in list(fi['feature'])]\n\nprint('%% of Top 100 Features created from the bureau data = %d.00' % len(new_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae9e0771495bdfff89f221f435c5320c8fd27f0"},"cell_type":"markdown","source":"Over half of the top 100 features were made by us! That should give us confidence that all the hard work we did was worthwhile. ","execution_count":null},{"metadata":{"_uuid":"0fe38b12afa98f44ad2a1e20da417f3e1f3728fd","collapsed":true,"trusted":true},"cell_type":"code","source":"submission_raw.to_csv('test_one.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bff36a22e3ee286f046f819bcca121c14df27a8"},"cell_type":"markdown","source":"__Test one scores 0.759 when submitted to the competition.__","execution_count":null},{"metadata":{"_uuid":"9469d920abeeb1c912391b06c1f4bd0175c15780"},"cell_type":"markdown","source":"### Test Two\n\nThat was easy, so let's do another run! Same as before but with the highly collinear variables removed. ","execution_count":null},{"metadata":{"_uuid":"008b1859c010277339ac2b1c2b52c0516c001774","trusted":true,"collapsed":true},"cell_type":"code","source":"submission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f91e580802f054053d497495de1c7aa4f4047eb","trusted":true,"collapsed":true},"cell_type":"code","source":"metrics_corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1c840d66cb195848d861133e24be78514624f30"},"cell_type":"markdown","source":"These results are better than the control, but slightly lower than the raw features. ","execution_count":null},{"metadata":{"_uuid":"ed8a71b2d72719418dcb9463dea983dfe2b1ade8","trusted":true,"collapsed":true},"cell_type":"code","source":"fi_corrs_sorted = plot_feature_importances(fi_corrs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"521e22fc1ba905c4851ca2d948260298a7527519","collapsed":true,"trusted":true},"cell_type":"code","source":"submission_corrs.to_csv('test_two.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cca331fe96700aa942ef56f96ce2fa34ad1933a"},"cell_type":"markdown","source":"__Test Two scores 0.753 when submitted to the competition.__","execution_count":null},{"metadata":{"_uuid":"457467aab25e8c047a2c74a1392730b96203cfa2"},"cell_type":"markdown","source":"# Results\n\nAfter all that work, we can say that including the extra information did improve performance! The model is definitely not optimized to our data, but we still had a noticeable improvement over the original dataset when using the calculated features. Let's officially summarize the performances:\n\n| __Experiment__ | __Train AUC__ | __Validation AUC__ | __Test AUC__  |\n|------------|-------|------------|-------|\n| __Control__    | 0.815 | 0.760      | 0.745 |\n| __Test One__   | 0.837 | 0.767      | 0.759 |\n| __Test Two__   | 0.826 | 0.765      | 0.753 |\n\n\n(Note that these scores may change from run to run of the notebook. I have not observed that the general ordering changes however.)\n\nAll of our hard work translates to a small improvement of 0.014 ROC AUC over the original testing data. Removing the highly collinear variables slightly decreases performance so we will want to consider a different method for feature selection. Moreover, we can say that some of the features we built are among the most important as judged by the model. \n\nIn a competition such as this, even an improvement of this size is enough to move us up 100s of spots on the leaderboard. By making numerous small improvements such as in this notebook, we can gradually achieve better and better performance. I encourage others to use the results here to make their own improvements, and I will continue to document the steps I take to help others. \n\n## Next Steps\n\nGoing forward, we can now use the functions we developed in this notebook on the other datasets. There are still 4 other data files to use in our model! In the next notebook, we will incorporate the information from these other data files (which contain information on previous loans at Home Credit) into our training data. Then we can build the same model and run more experiments to determine the effect of our feature engineering. There is plenty more work to be done in this competition, and plenty more gains in performance to be had! I'll see you in the next notebook.","execution_count":null},{"metadata":{"_uuid":"94bdea7d6198e5cf61f073c3c4d0386d5961b564","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}